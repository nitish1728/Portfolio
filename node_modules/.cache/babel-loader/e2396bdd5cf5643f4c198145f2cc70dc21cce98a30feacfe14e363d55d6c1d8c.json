{"ast":null,"code":"import { HfInference } from '@huggingface/inference';\nconst SYSTEM_PROMPT = `\nYou are an assistant that receives a list of ingredients that a user \nmake with some or all of those ingredients. You don't need to use every ingredient they mention\nin your receipe. The receipe can inculde additional ingredients they didn't mention, nbut try not to include too many\nextra ingredients.Format your response in markdown to make it easier to render  to a web page`;\nconst hf = new HfInference(process.env.REACT_APP_API_KEY);\nconsole.log(hf);\nexport async function generateReceipe(ingredientsArr) {\n  const ingredientsStr = ingredientsArr.join(\", \");\n  try {\n    const response = await hf.chatCompletion({\n      model: \"mistralai/Mistral-8xB-Instruct-v0.3\",\n      messages: [{\n        role: \"system\",\n        content: SYSTEM_PROMPT\n      }, {\n        role: \"user\",\n        content: `I have ${ingredientsStr}.Please give me a receipe \n                you would recommend I make`\n      }],\n      max_tokens: 1024\n    });\n    return response.choices[0].message.content;\n  } catch (e) {\n    console.error(e);\n  }\n}","map":{"version":3,"names":["HfInference","SYSTEM_PROMPT","hf","process","env","REACT_APP_API_KEY","console","log","generateReceipe","ingredientsArr","ingredientsStr","join","response","chatCompletion","model","messages","role","content","max_tokens","choices","message","e","error"],"sources":["C:/Users/barat/Documents/Projects/React JS/project4/src/components/javascript/main.js"],"sourcesContent":["import { HfInference } from '@huggingface/inference'\r\n\r\n\r\nconst SYSTEM_PROMPT=`\r\nYou are an assistant that receives a list of ingredients that a user \r\nmake with some or all of those ingredients. You don't need to use every ingredient they mention\r\nin your receipe. The receipe can inculde additional ingredients they didn't mention, nbut try not to include too many\r\nextra ingredients.Format your response in markdown to make it easier to render  to a web page`\r\nconst hf= new HfInference(process.env.REACT_APP_API_KEY)\r\nconsole.log(hf)\r\n\r\nexport async function generateReceipe(ingredientsArr){\r\n    const ingredientsStr=ingredientsArr.join(\", \")\r\n    try{\r\n        const response=await hf.chatCompletion({\r\n            model:\"mistralai/Mistral-8xB-Instruct-v0.3\",\r\n            messages:[\r\n                {role:\"system\",content:SYSTEM_PROMPT},\r\n                {role:\"user\",content:`I have ${ingredientsStr}.Please give me a receipe \r\n                you would recommend I make`}    \r\n            ],\r\n            max_tokens:1024\r\n        })\r\n        return response.choices[0].message.content\r\n    }\r\n    catch(e){\r\n        console.error(e)\r\n    }\r\n}"],"mappings":"AAAA,SAASA,WAAW,QAAQ,wBAAwB;AAGpD,MAAMC,aAAa,GAAC;AACpB;AACA;AACA;AACA,8FAA8F;AAC9F,MAAMC,EAAE,GAAE,IAAIF,WAAW,CAACG,OAAO,CAACC,GAAG,CAACC,iBAAiB,CAAC;AACxDC,OAAO,CAACC,GAAG,CAACL,EAAE,CAAC;AAEf,OAAO,eAAeM,eAAeA,CAACC,cAAc,EAAC;EACjD,MAAMC,cAAc,GAACD,cAAc,CAACE,IAAI,CAAC,IAAI,CAAC;EAC9C,IAAG;IACC,MAAMC,QAAQ,GAAC,MAAMV,EAAE,CAACW,cAAc,CAAC;MACnCC,KAAK,EAAC,qCAAqC;MAC3CC,QAAQ,EAAC,CACL;QAACC,IAAI,EAAC,QAAQ;QAACC,OAAO,EAAChB;MAAa,CAAC,EACrC;QAACe,IAAI,EAAC,MAAM;QAACC,OAAO,EAAC,UAAUP,cAAc;AAC7D;MAA2C,CAAC,CAC/B;MACDQ,UAAU,EAAC;IACf,CAAC,CAAC;IACF,OAAON,QAAQ,CAACO,OAAO,CAAC,CAAC,CAAC,CAACC,OAAO,CAACH,OAAO;EAC9C,CAAC,CACD,OAAMI,CAAC,EAAC;IACJf,OAAO,CAACgB,KAAK,CAACD,CAAC,CAAC;EACpB;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}